{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4437744,"sourceType":"datasetVersion","datasetId":2598560},{"sourceId":6616335,"sourceType":"datasetVersion","datasetId":3818337},{"sourceId":11408493,"sourceType":"datasetVersion","datasetId":7146365}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":458.289037,"end_time":"2023-11-17T20:30:57.033262","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-11-17T20:23:18.744225","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This weapon detection tool was developed as part of **Chapter 5: Practical AI in Cybersecurity** of my Master's thesis titled *Practical AI in Cyberwarfare and Cybersecurity*.  \nThe project was implemented by **Konstantinos Zafeiropoulos** (Student ID: `20390293`) at the  \n**University of West Attica**  \n**Faculty of Engineering, Department of Informatics and Computer Engineering**\n\nThe system uses a **dual-output deep learning model** based on **NASNetMobile** to perform both **weapon classification** and **bounding box localization**. It combines and preprocesses annotations from **YOLO** and **XML-formatted datasets** into a single, unified training set.\n\n**Key features include**:  \n- Multi-output architecture: classification + bounding box regression  \n- Bounding box normalization and annotation validation  \n- Custom data generator with real-time image preprocessing  \n- Live prediction using **OpenCV** and **TensorFlow**\n\nAlthough fully implemented, this tool was **not included in the final thesis submission** due to **time and scope constraints** and the decision to prioritize other tools with **higher experimental impact** and better alignment with the **core thesis narrative**.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\nimport os\nfrom PIL import Image\n\nimport pandas as pd\nfrom xml.dom import minidom\nimport csv\n\nimage_dir='/kaggle/input/weapon-detection-datasett/Sohas_weapon-Detection/images'\nannot_dir='/kaggle/input/weapon-detection-datasett/Sohas_weapon-Detection/annotations/xmls'","metadata":{"papermill":{"duration":1.678482,"end_time":"2023-11-17T20:23:30.082454","exception":false,"start_time":"2023-11-17T20:23:28.403972","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T21:03:12.194869Z","iopub.execute_input":"2025-04-14T21:03:12.195233Z","iopub.status.idle":"2025-04-14T21:03:14.199696Z","shell.execute_reply.started":"2025-04-14T21:03:12.195192Z","shell.execute_reply":"2025-04-14T21:03:14.198640Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\n\n# Automatically detect the correct path\nbase_path = '/kaggle/input/weapon-dataset-for-yolov5/dataset'\n\n# Drill into the actual dataset folder\nfor item in os.listdir(base_path):\n    if 'images' in os.listdir(os.path.join(base_path, item)):\n        dataset_root = os.path.join(base_path, item)\n\ntrain_images_dir = os.path.join(dataset_root, 'images/train')\ntrain_labels_dir = os.path.join(dataset_root, 'labels/train')\nval_images_dir = os.path.join(dataset_root, 'images/val')\nval_labels_dir = os.path.join(dataset_root, 'labels/val')\n\n# Function to convert YOLO-format .txt annotations to DataFrame\ndef convert_yolo_to_df(images_dir, labels_dir):\n    data = []\n    for label_file in os.listdir(labels_dir):\n        if not label_file.endswith('.txt'):\n            continue\n        img_filename = label_file.replace('.txt', '.jpg')  # assumes .jpg format\n        img_path = os.path.join(images_dir, img_filename)\n        label_path = os.path.join(labels_dir, label_file)\n        if not os.path.exists(img_path):\n            continue\n\n        # Get image dimensions\n        with Image.open(img_path) as img:\n            width, height = img.size\n\n        with open(label_path, 'r') as f:\n            for line in f.readlines():\n                parts = line.strip().split()\n                if len(parts) != 5:\n                    continue\n                class_id, x_center, y_center, w, h = map(float, parts)\n\n                # ALL classes in YOLO (0 = knife, 1 = handgun) become class_num = 1 (Weapon)\n                class_num = 1\n\n                xmin = int((x_center - w / 2) * width)\n                ymin = int((y_center - h / 2) * height)\n                xmax = int((x_center + w / 2) * width)\n                ymax = int((y_center + h / 2) * height)\n\n                data.append([\n                    img_filename, width, height, class_num,\n                    max(0, xmin), max(0, ymin), min(width, xmax), min(height, ymax)\n                ])\n    return pd.DataFrame(data, columns=[\n        'file_name', 'width', 'height', 'class_num',\n        'xmin', 'ymin', 'xmax', 'ymax'\n    ])\n\n# Run the conversion\nyolo_train_df = convert_yolo_to_df(train_images_dir, train_labels_dir)\nyolo_val_df = convert_yolo_to_df(val_images_dir, val_labels_dir)\n\n# Preview results\nprint(\"✅ YOLO training annotations loaded:\", len(yolo_train_df))\nprint(\"✅ YOLO validation annotations loaded:\", len(yolo_val_df))\nyolo_train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:04:02.308040Z","iopub.execute_input":"2025-04-14T22:04:02.308444Z","iopub.status.idle":"2025-04-14T22:04:16.297312Z","shell.execute_reply.started":"2025-04-14T22:04:02.308412Z","shell.execute_reply":"2025-04-14T22:04:16.296468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef rescaling(path_image, targetSize, xmin, ymin, xmax, ymax):\n    # Read the image in color (3 channels: BGR)\n    imageToPredict = cv2.imread(path_image, 3)\n\n    # Get original image dimensions\n    height = imageToPredict.shape[0]  # Number of rows (height)\n    width = imageToPredict.shape[1]   # Number of columns (width)\n\n    # Compute scaling factors for resizing\n    x_scale = targetSize / width\n    y_scale = targetSize / height\n\n    # Resize the image to (targetSize x targetSize)\n    img = cv2.resize(imageToPredict, (targetSize, targetSize))\n\n    # Convert the image to a NumPy array (ensures compatibility with models)\n    img = np.array(img)\n\n    # Save original bounding box values\n    origLeft = xmin\n    origTop = ymin\n    origRight = xmax\n    origBottom = ymax\n\n    # Scale the bounding box coordinates according to the new image size\n    xmin = int(np.round(origLeft * x_scale))\n    ymin = int(np.round(origTop * y_scale))\n    xmax = int(np.round(origRight * x_scale))\n    ymax = int(np.round(origBottom * y_scale))\n\n    # Return the resized image and the adjusted bounding box\n    return img, xmin, ymin, xmax, ymax","metadata":{"papermill":{"duration":0.048645,"end_time":"2023-11-17T20:23:30.248236","exception":false,"start_time":"2023-11-17T20:23:30.199591","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:04:23.543504Z","iopub.execute_input":"2025-04-14T22:04:23.544135Z","iopub.status.idle":"2025-04-14T22:04:23.550748Z","shell.execute_reply.started":"2025-04-14T22:04:23.544073Z","shell.execute_reply":"2025-04-14T22:04:23.549850Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def exteract_xml_contents(annot_directory, image_dir, target_size=300):\n    # Parse the XML annotation file\n    file = minidom.parse(annot_directory)\n\n    # Read the image and get its original height and width\n    height, width = cv2.imread(image_dir).shape[:2]\n\n    # Extract bounding box coordinates from the XML\n    xmin = file.getElementsByTagName('xmin')\n    x1 = float(xmin[0].firstChild.data)\n\n    ymin = file.getElementsByTagName('ymin')\n    y1 = float(ymin[0].firstChild.data)\n\n    xmax = file.getElementsByTagName('xmax')\n    x2 = float(xmax[0].firstChild.data)\n\n    ymax = file.getElementsByTagName('ymax')\n    y2 = float(ymax[0].firstChild.data)\n\n    # Extract the object class name (e.g., 'knife') from the XML\n    class_name = file.getElementsByTagName('name')\n    \n    # Accept multiple weapon keywords\n    weapon_keywords = ['knife', 'handgun', 'pistol', 'gun']\n    if class_name[0].firstChild.data.lower() in weapon_keywords:\n        class_num = 1\n    else:\n        class_num = 0\n\n    # Extract the image filename from the XML\n    files = file.getElementsByTagName('filename')\n    file_name = files[0].firstChild.data\n\n    # Resize the image and adjust the bounding box coordinates accordingly\n    img, xmin, ymin, xmax, ymax = rescaling(image_dir, target_size, x1, y1, x2, y2)\n\n    # Get the new height and width of the resized image\n    width = img.shape[0]\n    height = img.shape[1]\n\n    # Return all the extracted and processed information\n    return file_name, width, height, class_num, xmin, ymin, xmax, ymax","metadata":{"papermill":{"duration":0.050893,"end_time":"2023-11-17T20:23:30.334214","exception":false,"start_time":"2023-11-17T20:23:30.283321","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:04:25.989586Z","iopub.execute_input":"2025-04-14T22:04:25.990036Z","iopub.status.idle":"2025-04-14T22:04:26.001028Z","shell.execute_reply.started":"2025-04-14T22:04:25.989995Z","shell.execute_reply":"2025-04-14T22:04:26.000130Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def xml_to_csv(image_dir, annot_dir):\n    # Initialize a list to hold data extracted from each image + its XML file\n    xml_list = []\n\n    # Get all filenames from the annotations directory and the image directory\n    mat_files = os.listdir(annot_dir)\n    img_files = os.listdir(image_dir)\n\n    # Loop through all image files\n    for i, image in enumerate(img_files):\n        # Split the image filename to remove the extension (e.g., .jpg or .png)\n        xp = image.split('.')\n\n        # Build the full path to the corresponding XML annotation file\n        mat_path = os.path.join(annot_dir, (str(xp[0]) + '.xml'))\n\n        # Build the full path to the image file\n        img_path = os.path.join(image_dir, image)\n\n        # Call the exteract_xml_contents function to extract info from XML and image\n        value = exteract_xml_contents(mat_path, img_path)\n\n        # Append the extracted info (as a tuple/list) to the list\n        xml_list.append(value)\n\n    # Define the column names for the final DataFrame\n    columns_name = ['file_name', 'width', 'height', 'class_num', 'xmin', 'ymin', 'xmax', 'ymax']\n\n    # Create a Pandas DataFrame from the list of extracted information\n    xml_df = pd.DataFrame(xml_list, columns=columns_name)\n\n    # Return the final structured DataFrame\n    return xml_df","metadata":{"papermill":{"duration":0.050949,"end_time":"2023-11-17T20:23:30.420838","exception":false,"start_time":"2023-11-17T20:23:30.369889","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:04:28.419563Z","iopub.execute_input":"2025-04-14T22:04:28.419940Z","iopub.status.idle":"2025-04-14T22:04:28.426715Z","shell.execute_reply.started":"2025-04-14T22:04:28.419907Z","shell.execute_reply":"2025-04-14T22:04:28.425801Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract annotation and image data into a Pandas DataFrame\ntrain_labels_df = xml_to_csv(image_dir, annot_dir)\n\n# Save the DataFrame to a CSV file called 'dataset.csv' (without row indices)\ntrain_labels_df.to_csv('dataset.csv', index=None)\n\n# Display the DataFrame in interactive environments\ntrain_labels_df","metadata":{"papermill":{"duration":39.984843,"end_time":"2023-11-17T20:24:10.441012","exception":false,"start_time":"2023-11-17T20:23:30.456169","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:04:30.635164Z","iopub.execute_input":"2025-04-14T22:04:30.635564Z","iopub.status.idle":"2025-04-14T22:04:48.908612Z","shell.execute_reply.started":"2025-04-14T22:04:30.635523Z","shell.execute_reply":"2025-04-14T22:04:48.907254Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Combine original XML-based and YOLO-based annotations\ncombined_df = pd.concat([train_labels_df, yolo_train_df], ignore_index=True)\n\n# 2. Filter out invalid or very small bounding boxes\ncombined_df = combined_df[\n    (combined_df['xmax'] > combined_df['xmin'] + 5) &\n    (combined_df['ymax'] > combined_df['ymin'] + 5)\n]\ncombined_df = combined_df[\n    ~((combined_df['xmin'] == 0) & (combined_df['ymin'] == 0))\n]\n\n# 3. Shuffle the dataset to mix entries from both sources\ncombined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# 4. Save the cleaned dataset to a CSV file\ncombined_df.to_csv(\"final_combined_dataset.csv\", index=False)\nprint(\"✅ Cleaned and combined dataset saved as final_combined_dataset.csv\")\nprint(\"Total annotations after filtering:\", len(combined_df))\nprint(\"Unique images:\", combined_df['file_name'].nunique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:04:52.068422Z","iopub.execute_input":"2025-04-14T22:04:52.068801Z","iopub.status.idle":"2025-04-14T22:04:52.098615Z","shell.execute_reply.started":"2025-04-14T22:04:52.068772Z","shell.execute_reply":"2025-04-14T22:04:52.097533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"📦 YOLO dataset\")\nprint(yolo_train_df['class_num'].value_counts())\nprint(yolo_train_df.head())\n\nprint(\"\\n📦 XML dataset\")\nprint(train_labels_df['class_num'].value_counts())\nprint(train_labels_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:04:54.947993Z","iopub.execute_input":"2025-04-14T22:04:54.948393Z","iopub.status.idle":"2025-04-14T22:04:54.960341Z","shell.execute_reply.started":"2025-04-14T22:04:54.948359Z","shell.execute_reply":"2025-04-14T22:04:54.959244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a sorted list of class labels (this will be useful for encoding labels)\nclass_list = ['no weapon', 'weapon']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:04:59.296643Z","iopub.execute_input":"2025-04-14T22:04:59.297585Z","iopub.status.idle":"2025-04-14T22:04:59.301865Z","shell.execute_reply.started":"2025-04-14T22:04:59.297544Z","shell.execute_reply":"2025-04-14T22:04:59.300847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_dataset(csv_file, image_dir_1, image_dir_2):\n    image_paths = []\n    labels = []\n    boxes = []\n    none = {}\n\n    with open(csv_file) as csvfile:\n        rows = csv.reader(csvfile)\n        columns = next(iter(rows))\n\n        for i, row in enumerate(rows):\n            img_path = row[0]\n\n            # Try image_dir_1, fallback to image_dir_2\n            full_path = os.path.join(image_dir_1, img_path)\n            if not os.path.exists(full_path):\n                full_path = os.path.join(image_dir_2, img_path)\n\n            if not os.path.exists(full_path):\n                none[i] = full_path\n                continue\n\n            image_paths.append(full_path)\n            labels.append(int(row[3]))\n\n            arr = [\n                float(row[4]) / 300,\n                float(row[5]) / 300,\n                float(row[6]) / 300,\n                float(row[7]) / 300\n            ]\n            boxes.append(arr)\n\n    return image_paths, labels, boxes, none","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:05:01.524633Z","iopub.execute_input":"2025-04-14T22:05:01.525416Z","iopub.status.idle":"2025-04-14T22:05:01.532729Z","shell.execute_reply.started":"2025-04-14T22:05:01.525379Z","shell.execute_reply":"2025-04-14T22:05:01.531581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_paths, labels, boxes, none = preprocess_dataset(\n    \"final_combined_dataset.csv\",\n    '/kaggle/input/weapon-detection-datasett/Sohas_weapon-Detection/images',\n    '/kaggle/input/weapon-dataset-for-yolov5/dataset/dataset/images/train'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:05:03.788844Z","iopub.execute_input":"2025-04-14T22:05:03.789241Z","iopub.status.idle":"2025-04-14T22:05:04.124608Z","shell.execute_reply.started":"2025-04-14T22:05:03.789207Z","shell.execute_reply":"2025-04-14T22:05:04.123596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Matplotlib figure\nplt.figure(figsize=(15, 20))\n\n# Use a sample of 20 random indices from the available image_paths\nrandom_range = random.sample(range(len(image_paths)), 20)\n\nfor itr, i in enumerate(random_range, 1):\n    # Get the image path and load it\n    img_path = image_paths[i]\n    img = cv2.imread(img_path)\n    \n    if img is None:\n        continue  # Skip if image failed to load\n\n    # Resize image to 300x300 and normalize it\n    img = cv2.resize(img, (300, 300))\n    img = img.astype(\"float32\") / 255.0\n\n\n    # Extract bounding box and scale it to match the image size\n    a1, b1, a2, b2 = boxes[i]\n    x1 = int(a1 * 300)\n    y1 = int(b1 * 300)\n    x2 = int(a2 * 300)\n    y2 = int(b2 * 300)\n\n    # Draw the bounding box\n    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    # Plot the image\n    plt.subplot(4, 5, itr)\n    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:05:06.460420Z","iopub.execute_input":"2025-04-14T22:05:06.461171Z","iopub.status.idle":"2025-04-14T22:05:07.987709Z","shell.execute_reply.started":"2025-04-14T22:05:06.461135Z","shell.execute_reply":"2025-04-14T22:05:07.986484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"🔢 Total images:\", len(image_paths))\nprint(\"🟩 Total labels:\", len(labels))\nprint(\"📦 Total bounding boxes:\", len(boxes))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:05:25.113632Z","iopub.execute_input":"2025-04-14T22:05:25.114004Z","iopub.status.idle":"2025-04-14T22:05:25.119009Z","shell.execute_reply.started":"2025-04-14T22:05:25.113971Z","shell.execute_reply":"2025-04-14T22:05:25.118167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_image_paths, val_image_paths, train_labels, val_labels, train_boxes, val_boxes = train_test_split(\n    image_paths, labels, boxes, test_size=0.1, random_state=43\n)\n\nprint('✅ Split complete.')\nprint(f'Training: {len(train_image_paths)}, Validation: {len(val_image_paths)}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:05:27.235681Z","iopub.execute_input":"2025-04-14T22:05:27.236273Z","iopub.status.idle":"2025-04-14T22:05:27.245475Z","shell.execute_reply.started":"2025-04-14T22:05:27.236238Z","shell.execute_reply":"2025-04-14T22:05:27.244472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence\nimport cv2\nimport numpy as np\n\nclass WeaponDataGenerator(Sequence):\n    def __init__(self, image_paths, labels, boxes, batch_size=8, image_size=300, shuffle=True):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.boxes = boxes\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.shuffle = shuffle\n        self.indices = np.arange(len(self.image_paths))\n        self.on_epoch_end()\n\n    def __len__(self):\n        # Number of batches per epoch\n        return int(np.ceil(len(self.image_paths) / self.batch_size))\n\n    def __getitem__(self, index):\n        # Get batch indices\n        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n\n        # Initialize batches\n        batch_images = []\n        batch_labels = []\n        batch_boxes = []\n\n        # Process each image in the batch\n        for i in batch_indices:\n            img = cv2.imread(self.image_paths[i])\n            img = cv2.resize(img, (self.image_size, self.image_size))\n            img = img.astype(\"float32\") / 255.0\n\n            batch_images.append(img)\n            batch_labels.append(self.labels[i])\n            batch_boxes.append(self.boxes[i])\n\n        # Convert to NumPy arrays\n        batch_images = np.array(batch_images)\n        batch_labels = np.array(batch_labels)\n        batch_boxes = np.array(batch_boxes)\n\n        return batch_images, {\n            \"class_output\": batch_labels,\n            \"box_output\": batch_boxes\n        }\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:05:29.672241Z","iopub.execute_input":"2025-04-14T22:05:29.672616Z","iopub.status.idle":"2025-04-14T22:05:29.682648Z","shell.execute_reply.started":"2025-04-14T22:05:29.672584Z","shell.execute_reply":"2025-04-14T22:05:29.681639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator = WeaponDataGenerator(train_image_paths, train_labels, train_boxes, batch_size=8)\nval_generator = WeaponDataGenerator(val_image_paths, val_labels, val_boxes, batch_size=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:05:32.214201Z","iopub.execute_input":"2025-04-14T22:05:32.214557Z","iopub.status.idle":"2025-04-14T22:05:32.219473Z","shell.execute_reply.started":"2025-04-14T22:05:32.214527Z","shell.execute_reply":"2025-04-14T22:05:32.218465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras \nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Input\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.layers import GlobalAveragePooling2D,Dropout\nfrom tensorflow.keras.optimizers import SGD\nimage_size=300","metadata":{"papermill":{"duration":9.555976,"end_time":"2023-11-17T20:24:58.516394","exception":false,"start_time":"2023-11-17T20:24:48.960418","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:05:34.045203Z","iopub.execute_input":"2025-04-14T22:05:34.045570Z","iopub.status.idle":"2025-04-14T22:05:34.050610Z","shell.execute_reply.started":"2025-04-14T22:05:34.045531Z","shell.execute_reply":"2025-04-14T22:05:34.049506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a NASNetMobile model as a base feature extractor (pretrained CNN)\n\nN_mobile = tf.keras.applications.NASNetMobile(\n    input_tensor=Input(shape=(image_size, image_size, 3)),  # Define the input shape (300x300 RGB image)\n    include_top=False,                                      # Exclude the top classification layer (we’ll add our own output layers)\n    weights='imagenet'                                      # Load pretrained weights trained on ImageNet\n)","metadata":{"papermill":{"duration":9.900473,"end_time":"2023-11-17T20:25:08.479272","exception":false,"start_time":"2023-11-17T20:24:58.578799","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:05:36.317529Z","iopub.execute_input":"2025-04-14T22:05:36.317886Z","iopub.status.idle":"2025-04-14T22:05:39.604583Z","shell.execute_reply.started":"2025-04-14T22:05:36.317856Z","shell.execute_reply":"2025-04-14T22:05:39.603600Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model(no_of_classes):\n\n    # Freeze all layers in NASNetMobile to prevent them from being updated during training\n    N_mobile.trainable = False\n\n    # Get the output of the base NASNetMobile model\n    base_model_output = N_mobile.output\n\n    # Apply Global Average Pooling to reduce the 4D feature map to a 2D vector\n    flattened_output = GlobalAveragePooling2D()(base_model_output)\n\n    # Just for debugging: print the shape of the pooled feature vector\n    print(flattened_output.shape)\n\n    # ====== Classification Branch ======\n\n    # Fully connected layers to predict the class of the object\n    class_prediction = Dense(256, activation=\"relu\")(flattened_output)\n    class_prediction = Dense(128, activation=\"relu\")(class_prediction)\n    class_prediction = Dropout(0.2)(class_prediction)  # Dropout to reduce overfitting\n    class_prediction = Dense(64, activation=\"relu\")(class_prediction)\n    class_prediction = Dropout(0.2)(class_prediction)\n    class_prediction = Dense(32, activation=\"relu\")(class_prediction)\n\n    # Final classification output layer (softmax for multi-class classification)\n    class_prediction = Dense(no_of_classes, activation='softmax', name=\"class_output\")(class_prediction)\n\n    # ====== Bounding Box Regression Branch ======\n\n    # Fully connected layers to predict bounding box coordinates (xmin, ymin, xmax, ymax)\n    box_output = Dense(256, activation=\"relu\")(flattened_output)\n    box_output = Dense(128, activation=\"relu\")(box_output)\n    box_output = Dropout(0.2)(box_output)\n    box_output = Dense(64, activation=\"relu\")(box_output)\n    box_output = Dropout(0.2)(box_output)\n    box_output = Dense(32, activation=\"relu\")(box_output)\n\n    # Final output layer for bounding boxes — 4 values (all between 0 and 1 using sigmoid)\n    box_predictions = Dense(4, activation='sigmoid', name=\"box_output\")(box_output)\n\n    # Create a Keras Model that takes the NASNet input and gives two outputs:\n    # one for bounding box and one for class prediction\n    model = Model(inputs=N_mobile.input, outputs=[box_predictions, class_prediction])\n\n    return model","metadata":{"papermill":{"duration":0.081186,"end_time":"2023-11-17T20:25:08.625221","exception":false,"start_time":"2023-11-17T20:25:08.544035","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:05:44.672848Z","iopub.execute_input":"2025-04-14T22:05:44.673484Z","iopub.status.idle":"2025-04-14T22:05:44.681548Z","shell.execute_reply.started":"2025-04-14T22:05:44.673447Z","shell.execute_reply":"2025-04-14T22:05:44.680407Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = create_model(2)\nmodel","metadata":{"papermill":{"duration":0.341446,"end_time":"2023-11-17T20:25:09.03087","exception":false,"start_time":"2023-11-17T20:25:08.689424","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:05:47.365744Z","iopub.execute_input":"2025-04-14T22:05:47.366609Z","iopub.status.idle":"2025-04-14T22:05:47.546457Z","shell.execute_reply.started":"2025-04-14T22:05:47.366563Z","shell.execute_reply":"2025-04-14T22:05:47.545447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the loss functions for each output of the model.\n# - \"box_output\": uses mean squared error (MSE) for bounding box regression.\n# - \"class_output\": uses sparse categorical crossentropy for multi-class classification (labels are integers).\nlosses = { \n    \"box_output\": \"mean_squared_error\",\n    \"class_output\": \"sparse_categorical_crossentropy\"\n}\n\n# Set the importance (weight) of each loss during training.\n# Here, both outputs are equally important with a weight of 1.0 each.\nloss_weights = {\n    \"box_output\": 1.0, \n    \"class_output\": 1.0\n}\n\n# Specify evaluation metrics for each output:\n# - \"class_output\": monitor classification accuracy.\n# - \"box_output\": monitor mean squared error (MSE) of bounding box predictions.\nmetrics = {\n    \"class_output\": \"accuracy\", \n    \"box_output\":  \"mse\"\n}\n\n# Use EarlyStopping to stop training early if the validation loss doesn't improve.\n# - It waits 40 epochs (patience) for improvement.\n# - When stopping, it restores the best weights seen during training.\nstop = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",        # Monitor validation loss\n    min_delta=0.0001,          # Minimum change to qualify as an improvement\n    patience=40,               # Stop after 40 epochs without improvement\n    restore_best_weights=True # Load the best weights before stopping\n)\n\n# Use ReduceLROnPlateau to reduce the learning rate if validation loss stops improving.\n# - Helps the model converge more smoothly by lowering learning rate when stuck.\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",       # Monitor validation loss\n    factor=0.0002,            # Reduce learning rate by this factor\n    patience=30,              # Wait 30 epochs without improvement before reducing\n    min_lr=1e-7,              # Don't go lower than this learning rate\n    verbose=1                 # Print when learning rate is updated\n)\n\n# Initialize the optimizer: Stochastic Gradient Descent (SGD)\n# - learning_rate = 0.001\n# - momentum = 0.9 for smoother convergence\nopt = SGD(learning_rate=1e-3, momentum=0.9)\n\n# Compile the model with:\n# - SGD optimizer\n# - Specified losses for each output\n# - Loss weights to balance them\n# - Metrics to monitor during training\nmodel.compile(\n    optimizer=opt,\n    loss=losses,\n    loss_weights=loss_weights,\n    metrics=metrics\n)","metadata":{"papermill":{"duration":0.12385,"end_time":"2023-11-17T20:25:09.217858","exception":false,"start_time":"2023-11-17T20:25:09.094008","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2025-04-14T22:05:50.343976Z","iopub.execute_input":"2025-04-14T22:05:50.344797Z","iopub.status.idle":"2025-04-14T22:05:50.357681Z","shell.execute_reply.started":"2025-04-14T22:05:50.344746Z","shell.execute_reply":"2025-04-14T22:05:50.356978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model using GPU (if available)\nwith tf.device('/device:GPU:0'):  # Use GPU for faster training\n    history = model.fit(\n        train_generator,            # Training generator\n        validation_data=val_generator,  # Validation generator\n        epochs=70,                  # Number of epochs\n        callbacks=[reduce_lr, stop]  # Learning rate scheduler and early stopping\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:06:02.971735Z","iopub.execute_input":"2025-04-14T22:06:02.972103Z","iopub.status.idle":"2025-04-14T22:39:18.420210Z","shell.execute_reply.started":"2025-04-14T22:06:02.972059Z","shell.execute_reply":"2025-04-14T22:39:18.419455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot(var1, var2, plot_name):\n    # Get the loss/metric history from model training\n    c1 = history.history[var1]\n    c2 = history.history[var2]\n\n    # Define the x-axis (epoch count)\n    epochs = range(len(c1))\n\n    # Plot both training and validation metrics\n    plt.figure(figsize=(8, 5))\n    plt.plot(epochs, c1, 'b', label=var1)\n    plt.plot(epochs, c2, 'r', label=var2)\n    plt.title(plot_name)\n    plt.xlabel('Epoch')\n    plt.ylabel('Value')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"papermill":{"duration":0.087735,"end_time":"2023-11-17T20:30:49.417521","exception":false,"start_time":"2023-11-17T20:30:49.329786","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:39:22.861656Z","iopub.execute_input":"2025-04-14T22:39:22.862024Z","iopub.status.idle":"2025-04-14T22:39:22.867919Z","shell.execute_reply.started":"2025-04-14T22:39:22.861991Z","shell.execute_reply":"2025-04-14T22:39:22.866891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot('class_output_accuracy', 'val_class_output_accuracy', 'Training vs Validation Accuracy')\nplot('box_output_mse', 'val_box_output_mse', 'Training vs Validation MSE (Bounding Boxes)')\nplot('loss', 'val_loss', 'Training vs Validation Total Loss')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:39:26.049918Z","iopub.execute_input":"2025-04-14T22:39:26.050525Z","iopub.status.idle":"2025-04-14T22:39:27.488752Z","shell.execute_reply.started":"2025-04-14T22:39:26.050490Z","shell.execute_reply":"2025-04-14T22:39:27.487952Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The three graphs indicate that the model classifies weapons with high accuracy while struggling slightly with bounding box precision. The classification accuracy quickly reaches 100% on training and stabilizes around 99.1% on validation, showing excellent generalization. The bounding box MSE steadily decreases but maintains a gap between training and validation, suggesting label inconsistencies or easier validation samples. \n\nThe total loss graph confirms this trend, with validation loss plateauing after epoch 35. Training beyond 40–50 epochs yields minimal gains and earlier stopping would likely maintain performance while reducing overfitting risk.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n# 👣 Step 1: Load and preprocess images\ndef load_and_preprocess_images(image_paths, target_size=(300, 300)):\n    processed_images = []\n    for path in image_paths:\n        img = cv2.imread(path)\n        if img is not None:\n            img = cv2.resize(img, target_size)\n            img = img.astype(\"float32\") / 255.0\n            processed_images.append(img)\n    return np.array(processed_images)\n\nval_images = load_and_preprocess_images(val_image_paths)\n\n# 👣 Step 2: Predict\ny_pred_probs = model.predict(val_images)[1]\ny_pred = np.argmax(y_pred_probs, axis=-1)\n\n# 👣 Step 3: True labels\ny_true = np.array(val_labels)\n\n# 👣 Step 4: Accuracy\naccuracy = accuracy_score(y_true, y_pred)\nprint(f\"✅ Classification Accuracy: {accuracy:.4f}\")\n\n# 👣 Step 5: Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_list, yticklabels=class_list)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:43:55.265598Z","iopub.execute_input":"2025-04-14T22:43:55.266423Z","iopub.status.idle":"2025-04-14T22:44:19.796865Z","shell.execute_reply.started":"2025-04-14T22:43:55.266376Z","shell.execute_reply":"2025-04-14T22:44:19.796045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# True labels and predictions (already computed)\n# y_true = val_labels\n# y_pred = np.argmax(model.predict(val_images)[1], axis=-1)\n\n# Compute metrics\naccuracy = accuracy_score(val_labels, y_pred)\nprecision = precision_score(val_labels, y_pred)\nrecall = recall_score(val_labels, y_pred)\nf1 = f1_score(val_labels, y_pred)\n\n# Print results\nprint(f\"✅ Accuracy:  {accuracy:.4f}\")\nprint(f\"✅ Precision: {precision:.4f}\")\nprint(f\"✅ Recall:    {recall:.4f}\")\nprint(f\"✅ F1 Score:  {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:45:20.635251Z","iopub.execute_input":"2025-04-14T22:45:20.635618Z","iopub.status.idle":"2025-04-14T22:45:20.650068Z","shell.execute_reply.started":"2025-04-14T22:45:20.635587Z","shell.execute_reply":"2025-04-14T22:45:20.649143Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Save your model here in .h5 format.\nmodel.save('caltech_normal.h5')","metadata":{"papermill":{"duration":1.315814,"end_time":"2023-11-17T20:30:52.450725","exception":false,"start_time":"2023-11-17T20:30:51.134911","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:45:23.770449Z","iopub.execute_input":"2025-04-14T22:45:23.771066Z","iopub.status.idle":"2025-04-14T22:45:24.806041Z","shell.execute_reply.started":"2025-04-14T22:45:23.771031Z","shell.execute_reply":"2025-04-14T22:45:24.805109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ONLY IF YOU WANT TO LOAD THE MODEL WITHOUT TRAINING IT AGAIN\nfrom tensorflow.keras.models import load_model\n\nmodel = load_model('caltech_normal.h5')\nprint(\"✅ Model loaded successfully\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global label_names\n \n# Must be same as the Annotations list we used\nlabel_names = sorted(class_list)","metadata":{"papermill":{"duration":0.089167,"end_time":"2023-11-17T20:30:52.618048","exception":false,"start_time":"2023-11-17T20:30:52.528881","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:45:28.360993Z","iopub.execute_input":"2025-04-14T22:45:28.361409Z","iopub.status.idle":"2025-04-14T22:45:28.366186Z","shell.execute_reply.started":"2025-04-14T22:45:28.361370Z","shell.execute_reply":"2025-04-14T22:45:28.365152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(img, image_size=300):\n    # Resize the image to the target size (e.g., 300x300) to match the input shape of the model\n    image = cv2.resize(img, (image_size, image_size))\n    \n    # Convert the color space from BGR (OpenCV default) to RGB (used by most ML models)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Normalize the pixel values by dividing by 255 to bring them in the range [0, 1]\n    # This is important because most models are trained with this normalization\n    image = image.astype(\"float\") / 255.0\n    \n    # Expand the dimensions of the image to include a batch size of 1\n    # This is necessary because models expect input to be in the format (batch_size, height, width, channels)\n    image = np.expand_dims(image, axis=0)\n    \n    return image","metadata":{"papermill":{"duration":0.088817,"end_time":"2023-11-17T20:30:52.955197","exception":false,"start_time":"2023-11-17T20:30:52.86638","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:45:30.461814Z","iopub.execute_input":"2025-04-14T22:45:30.462187Z","iopub.status.idle":"2025-04-14T22:45:30.467793Z","shell.execute_reply.started":"2025-04-14T22:45:30.462153Z","shell.execute_reply":"2025-04-14T22:45:30.466870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def postprocess(image, results):\n    # The model's predictions are split into two parts:\n    # - bounding_box: predicted bounding box coordinates (normalized between 0 and 1)\n    # - class_probs: predicted class probabilities (the likelihood of each class)\n    bounding_box, class_probs = results\n    \n    # Find the index of the class with the highest probability (most likely class)\n    class_index = np.argmax(class_probs)\n    \n    # Get the label of the predicted class using the index from the label_names list\n    class_label = label_names[class_index]\n    \n    # Get the height (h) and width (w) of the image to scale the bounding box coordinates\n    h, w = image.shape[:2]\n\n    # Extract the bounding box coordinates from the prediction\n    x1, y1, x2, y2 = bounding_box[0]\n    \n    # Convert the bounding box coordinates from relative values (0-1) to actual pixel values\n    x1 = int(w * x1)  # Convert x1 to pixel value based on width\n    x2 = int(w * x2)  # Convert x2 to pixel value based on width\n    y1 = int(h * y1)  # Convert y1 to pixel value based on height\n    y2 = int(h * y2)  # Convert y2 to pixel value based on height\n    \n    # Return the predicted class label, the converted bounding box coordinates, and the class probabilities\n    return class_label, (x1, y1, x2, y2), class_probs","metadata":{"papermill":{"duration":0.090243,"end_time":"2023-11-17T20:30:53.124108","exception":false,"start_time":"2023-11-17T20:30:53.033865","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:45:32.501662Z","iopub.execute_input":"2025-04-14T22:45:32.502441Z","iopub.status.idle":"2025-04-14T22:45:32.508277Z","shell.execute_reply.started":"2025-04-14T22:45:32.502405Z","shell.execute_reply":"2025-04-14T22:45:32.507322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(image, returnimage=False, scale=0.9):\n    # Process the input image\n    processed_image = preprocess(image)\n\n    # Predict with the model\n    results = model.predict(processed_image)\n\n    # Postprocess\n    label, (x1, y1, x2, y2), confidence = postprocess(image, results)\n\n    # Draw bounding box\n    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 100), 2)\n\n    # Format confidence\n    confidence_score = np.max(confidence)  # Get top class confidence\n\n    # Put label and confidence\n    cv2.putText(\n        image, \n        f'{label}: {confidence_score:.2f}',  # ✅ FIXED this line\n        (x1, y2 + 30), \n        cv2.FONT_HERSHEY_COMPLEX, \n        scale, \n        (200, 300, 100), \n        2\n    )\n\n    # Show image\n    plt.figure(figsize=(10, 10))\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.title(\"Prediction\")\n    plt.show()\n\n\n    # Optional: return image if needed\n    if returnimage:\n        return image","metadata":{"papermill":{"duration":0.092574,"end_time":"2023-11-17T20:30:52.788662","exception":false,"start_time":"2023-11-17T20:30:52.696088","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:45:34.736390Z","iopub.execute_input":"2025-04-14T22:45:34.736730Z","iopub.status.idle":"2025-04-14T22:45:34.743803Z","shell.execute_reply.started":"2025-04-14T22:45:34.736698Z","shell.execute_reply":"2025-04-14T22:45:34.742852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load your image\nimage = cv2.imread('/kaggle/input/knifeman/our.jpg')\n\n# 2. Preprocess it for the model\nprocessed_image = preprocess(image)\n\n# 3. Get model predictions\nresults = model.predict(processed_image)\n\n# 4. Post-process to extract label, box, and confidence\nlabel, (x1, y1, x2, y2), confidence = postprocess(image, results)\n\n# 5. Print details\nprint(\"🔍 Predicted label:\", label)\nprint(\"📦 Bounding Box:\", x1, y1, x2, y2)\nprint(\"✅ Confidence score:\", np.max(confidence))\n\n# 6. (Optional) Visualize the result manually\ncv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 100), 2)\ncv2.putText(image, f'{label}: {np.max(confidence):.2f}', (x1, y2 + 30),\n            cv2.FONT_HERSHEY_COMPLEX, 0.9, (200, 300, 100), 2)\nplt.figure(figsize=(10, 10))\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title(\"Prediction on Uploaded Image\")\nplt.show()\n","metadata":{"papermill":{"duration":0.077746,"end_time":"2023-11-17T20:30:53.279629","exception":false,"start_time":"2023-11-17T20:30:53.201883","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:55:23.058595Z","iopub.execute_input":"2025-04-14T22:55:23.059298Z","iopub.status.idle":"2025-04-14T22:55:23.562871Z","shell.execute_reply.started":"2025-04-14T22:55:23.059265Z","shell.execute_reply":"2025-04-14T22:55:23.561852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model we employed is a multi-output convolutional neural network (CNN) trained to perform two tasks simultaneously: binary classification (weapon vs. no weapon) and bounding box regression for localization. The classification branch analyzes the entire image to determine the presence of a weapon based on learned visual features such as shape and context, while the bounding box branch attempts to localize the weapon based on training annotations. In the image shown, the model correctly identifies the presence of a weapon with high confidence (confidence score = 1.0), but the bounding box is incorrectly placed, not surrounding the actual weapon. \n\nThis discrepancy highlights a known issue: while the classifier performs robustly by generalizing from global features, the bounding box predictor is highly sensitive to the quality of training labels. We retained this image as a clear demonstration of the model’s ability to classify correctly despite poor spatial localization, emphasizing that the classification branch functions independently from the accuracy of the bounding box data.","metadata":{}}]}